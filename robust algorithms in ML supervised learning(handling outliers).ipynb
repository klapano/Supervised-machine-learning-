{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86ba055f",
   "metadata": {},
   "source": [
    "1) Least Absolute Deviation (LAD) Linear Regression:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0331413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, TheilSenRegressor\n",
    "\n",
    "# LAD Linear Regression\n",
    "lad = TheilSenRegressor()\n",
    "lad.fit(X_train, y_train)\n",
    "y_pred = lad.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfe3582",
   "metadata": {},
   "source": [
    "2) Robust Random Forest:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5eae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Robust Random Forest\n",
    "rf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.01, criterion='mae',\n",
    "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "                      max_samples=None, min_impurity_decrease=0.0,\n",
    "                      min_impurity_split=None, min_samples_leaf=1,\n",
    "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                      n_estimators=100, n_jobs=-1, oob_score=False,\n",
    "                      random_state=0, verbose=0, warm_start=False)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2af2e1a",
   "metadata": {},
   "source": [
    "3) Huber Regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce20117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "# Huber Regressor\n",
    "huber = HuberRegressor()\n",
    "huber.fit(X_train, y_train)\n",
    "y_pred = huber.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4471cd09",
   "metadata": {},
   "source": [
    "4) RANSAC Regressor :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed99d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RANSACRegressor\n",
    "\n",
    "# RANSAC Regressor\n",
    "ransac = RANSACRegressor()\n",
    "ransac.fit(X_train, y_train)\n",
    "y_pred = ransac.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4284e8b7",
   "metadata": {},
   "source": [
    "It's worth noting that, in the above examples, X_train, y_train, X_test and y_test are the feature and label values of the training and testing sets. The choice of robust algorithm will depend on the specific problem, dataset and requirements of the model. It's important to evaluate the performance of the model and compare it with other algorithms to ensure that it's the best fit for the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a5bda7",
   "metadata": {},
   "source": [
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b317bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# load the boston housing dataset\n",
    "data = load_boston()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Identify the outliers\n",
    "#Identify the outliers: You'll need to first identify the outliers in your dataset. \n",
    "#You can use techniques such as the interquartile range (IQR) method or the Z-score method to identify outliers.\n",
    "\n",
    "q75, q25 = np.percentile(y, [75 ,25])\n",
    "iqr = q75 - q25\n",
    "min_val = q25 - (iqr*1.5)\n",
    "max_val = q75 + (iqr*1.5)\n",
    "\n",
    "# Remove the outliers\n",
    "outliers = y[(y < min_val) | (y > max_val)]\n",
    "X = X[(y >= min_val) & (y <= max_val)]\n",
    "y = y[(y >= min_val) & (y <= max_val)]\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "#Train the model: Use the preprocessed data to train the robust algorithm you've chosen. \n",
    "#Make sure to use cross-validation to evaluate the performance of the model.\n",
    "\n",
    "huber = HuberRegressor()\n",
    "huber.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "#Make predictions: Once the model has been trained, use it to make predictions on new, unseen data.\n",
    "\n",
    "y_pred = huber.predict(X_test)\n",
    "\n",
    "# evaluate the model\n",
    "#Evaluate the model: Evaluate the performance of the model using metrics such as accuracy, precision, and recall. \n",
    "#Compare the performance of the robust algorithm with other algorithms to ensure that it's the best fit for the problem at hand.\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error: \", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff66fe8",
   "metadata": {},
   "source": [
    "In this example, the Boston Housing dataset is loaded and the values are stored in X and y. Then, the outliers are identified using the interquartile range method. After that, the outliers are removed from the dataset, X_train, X_test, y_train, y_test are created by splitting the data into train and test sets. The HuberRegressor is used as the robust algorithm, it is trained on X_train and y_train, then predictions are made on X_test, and the performance of the model is evaluated by calculating the Mean Squared Error.\n",
    "\n",
    "It's important to note that the above example is for a simple regression problem, the process may vary depending on the specific problem and dataset. Also, the choice of robust algorithm will depend on the specific problem, dataset and requirements of the model. It's important to evaluate the performance of the model and compare it with other algorithms to ensure that it's the best fit for the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b1cbb9",
   "metadata": {},
   "source": [
    "# ----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cacddb7",
   "metadata": {},
   "source": [
    "The Z-score method is used to identify outliers by calculating the number of standard deviations a data point is from the mean. Data points with a Z-score greater than a certain threshold (usually 3 or -3) are considered outliers. Here's how you can use the Z-score method to identify outliers in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce0511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# calculate the z-scores\n",
    "z_scores = (data - np.mean(data)) / np.std(data)\n",
    "\n",
    "# identify the outliers\n",
    "outliers = data[np.abs(z_scores) > 3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ea827d",
   "metadata": {},
   "source": [
    "Once you have identified the outliers, you can preprocess the data by scaling or normalizing it. Scaling and normalizing the data can help to ensure that all features are on the same scale, which can improve the performance of some algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaa805e",
   "metadata": {},
   "source": [
    "# Here are some common techniques for scaling and normalizing data in python:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e93a11",
   "metadata": {},
   "source": [
    "1) Min-Max Scaling:\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2404bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# create the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit the scaler on the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform the training and test data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab9d35c",
   "metadata": {},
   "source": [
    "2) Standardization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab1020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit the scaler on the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform the training and test data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338bbc22",
   "metadata": {},
   "source": [
    "It's important to note that, in the above examples, X_train, X_test are the feature values of the training and testing sets. Also, it's important to scale or normalize the data after removing the outliers, to avoid that the model is affected by the removed values.\n",
    "\n",
    "It's worth noting that, not all algorithms require scaling or normalization, and it is important to know the algorithm you are working with and its requirements before applying scaling or normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa52f2f9",
   "metadata": {},
   "source": [
    "# Scaling and normalizing data can help to ensure that all features are on the same scale, which can improve the performance of some algorithms. However, not all algorithms require scaling or normalization. Some algorithms are scale-invariant, which means that they do not rely on the scale of the data, and therefore do not require scaling or normalization."
   ]
  },
  {
   "cell_type": "raw",
   "id": "552e53e7",
   "metadata": {},
   "source": [
    "Decision tree-based algorithms such as Random Forest and Gradient Boosting are scale-invariant, which means that they do not rely on the scale of the data, and therefore do not require scaling or normalization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ec0408",
   "metadata": {},
   "source": [
    "some algorithms like the k-neighbors algorithm are based on distance, this means that if all the features are not scaled, features with larger scales will dominate the distance calculation, resulting in poor performance."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e80c83e7",
   "metadata": {},
   "source": [
    "algorithms such as linear and logistic regression, neural networks and support vector machines, are not scale-invariant and require scaling or normalization of the data before training, this is because these algorithms rely on the scale of the data, and if the scale is not consistent, it will affect the model's ability to learn the relationship between the input and output variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bad4c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118a0904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f73d5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
